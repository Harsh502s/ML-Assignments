{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **µ-LMS approximate to Gradient Descent**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's briefly review the gradient descent algorithm. Gradient descent is an iterative optimization algorithm that minimizes a given function by iteratively adjusting its parameters in the opposite direction of the gradient of the function with respect to those parameters. This process continues until the algorithm converges to a minimum point of the function.\n",
    "\n",
    "Now, let's move on to µ-LMS. µ-LMS stands for Least Mean Squares. LMS is a commonly used algorithm in signal processing and machine learning for estimating unknown parameters in a linear system.\n",
    "\n",
    "µ-LMS is an approximate version of the gradient descent algorithm that is particularly well-suited for systems with a small number of parameters. The main idea behind µ-LMS is to approximate the gradient of the function being optimized using a single data point. This is in contrast to the full-batch gradient descent, which uses all the data points to calculate the gradient.\n",
    "\n",
    "Using only one data point to approximate the gradient has two benefits. First, it reduces the computational cost of the algorithm since we only need to compute the gradient with respect to a single data point instead of all data points. Second, it can lead to faster convergence, especially when the data is sparse."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm works by iteratively updating the parameters of the model using a single data point at each iteration. Let's take a closer look at the steps involved in the algorithm:\n",
    "\n",
    "Initialize the parameters of the model to some initial values.\n",
    "\n",
    "For each data point in the training set, perform the following steps:\n",
    "\n",
    "a. Compute the output of the model for the given input using the current set of parameters.\n",
    "\n",
    "b. Calculate the error between the predicted output and the actual output.\n",
    "\n",
    "c. Update the parameters of the model using the following formula:\n",
    "\n",
    "θ_i = θ_i - µ * error * x_i\n",
    "\n",
    "where θ_i is the i-th parameter of the model, x_i is the i-th feature of the input data point, and µ is a learning rate parameter that determines the step size of the updates.\n",
    "\n",
    "Repeat step 2 for a fixed number of iterations or until the algorithm converges to a minimum point.\n",
    "\n",
    "One important thing to note is that µ-LMS uses a constant learning rate µ for all iterations. In contrast, the standard gradient descent algorithm often uses a decreasing learning rate that decreases over time as the algorithm approaches a minimum point.\n",
    "\n",
    "The choice of the learning rate parameter µ is critical for the performance of the algorithm. If µ is too small, the algorithm may converge very slowly or not converge at all. If µ is too large, the algorithm may overshoot the minimum point and diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
