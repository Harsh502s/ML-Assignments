{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **µ-LMS approximate to Gradient Descent**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's briefly review the gradient descent algorithm. Gradient descent is an iterative optimization algorithm that minimizes a given function by iteratively adjusting its parameters in the opposite direction of the gradient of the function with respect to those parameters. This process continues until the algorithm converges to a minimum point of the function.\n",
    "\n",
    "Now, let's move on to µ-LMS. µ-LMS stands for micro-LMS, and LMS stands for Least Mean Squares. LMS is a commonly used algorithm in signal processing and machine learning for estimating unknown parameters in a linear system.\n",
    "\n",
    "µ-LMS is an approximate version of the gradient descent algorithm that is particularly well-suited for systems with a small number of parameters. The main idea behind µ-LMS is to approximate the gradient of the function being optimized using a single data point. This is in contrast to the full-batch gradient descent, which uses all the data points to calculate the gradient.\n",
    "\n",
    "Using only one data point to approximate the gradient has two benefits. First, it reduces the computational cost of the algorithm since we only need to compute the gradient with respect to a single data point instead of all data points. Second, it can lead to faster convergence, especially when the data is sparse."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm works by iteratively updating the parameters of the model using a single data point at each iteration. Let's take a closer look at the steps involved in the algorithm:\n",
    "\n",
    "Initialize the parameters of the model to some initial values.\n",
    "\n",
    "For each data point in the training set, perform the following steps:\n",
    "\n",
    "a. Compute the output of the model for the given input using the current set of parameters.\n",
    "\n",
    "b. Calculate the error between the predicted output and the actual output.\n",
    "\n",
    "c. Update the parameters of the model using the following formula:\n",
    "\n",
    "θ_i = θ_i - µ * error * x_i\n",
    "\n",
    "where θ_i is the i-th parameter of the model, x_i is the i-th feature of the input data point, and µ is a learning rate parameter that determines the step size of the updates.\n",
    "\n",
    "Repeat step 2 for a fixed number of iterations or until the algorithm converges to a minimum point.\n",
    "\n",
    "One important thing to note is that µ-LMS uses a constant learning rate µ for all iterations. In contrast, the standard gradient descent algorithm often uses a decreasing learning rate that decreases over time as the algorithm approaches a minimum point.\n",
    "\n",
    "The choice of the learning rate parameter µ is critical for the performance of the algorithm. If µ is too small, the algorithm may converge very slowly or not converge at all. If µ is too large, the algorithm may overshoot the minimum point and diverge."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can use the dataset to compare the performance of the µ-LMS algorithm with the standard gradient descent algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "data = \n",
    "X = data.d\n",
    "y = \n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the learning rate and number of iterations for the µ-LMS algorithm\n",
    "lr = 0.01\n",
    "n_iter = 1000\n",
    "\n",
    "# Fit a linear regression model using the standard gradient descent algorithm\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Compute the mean squared error on the testing set\n",
    "lr_mse = np.mean((lr_model.predict(X_test) - y_test) ** 2)\n",
    "\n",
    "# Implement the µ-LMS algorithm\n",
    "weights = np.zeros(X_train.shape[1])\n",
    "for i in range(n_iter):\n",
    "    # Select a random data point\n",
    "    j = np.random.randint(X_train.shape[0])\n",
    "    # Compute the predicted output for the data point\n",
    "    y_pred = np.dot(X_train[j], weights)\n",
    "    # Compute the error between the predicted output and the actual output\n",
    "    error = y_train[j] - y_pred\n",
    "    # Update the weights using the µ-LMS update rule\n",
    "    weights += lr * error * X_train[j]\n",
    "\n",
    "# Compute the mean squared error on the testing set\n",
    "ulms_mse = np.mean((np.dot(X_test, weights) - y_test) ** 2)\n",
    "\n",
    "# Compare the mean squared errors of the two models\n",
    "print(\"Linear Regression MSE:\", lr_mse)\n",
    "print(\"µ-LMS MSE:\", ulms_mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
